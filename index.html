<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="ImagenWorld: Stress-Testing Image Generation Models with Explainable Human Evaluation on Open-ended Real-World Tasks">
    <meta property="og:title" content="ImagenWorld: Stress-Testing Image Generation Models with Explainable Human Evaluation on Open-ended Real-World Tasks" />
    <meta property="og:description" content="We introduce ImagenWorld, a 3.6K-condition benchmark with 20K human annotations for evaluating image generation and editing across six tasks and domains, offering fine-grained, explainable error analysis beyond automated VLM metrics." />
    <meta property="og:url" content="https://github.com/TIGER-AI-Lab/ImagenWorld" />

    <title>ImagenWorld: Stress-Testing Image Generation Models with Explainable Human Evaluation on Open-ended Real-World Tasks</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"> ImagenWorld: Stress-Testing Image Generation Models with Explainable Human Evaluation on Open-ended Real-World Tasks</h1>
                        <img src="static/images/psudo_banner.png" width="75%" alt="ImagenWorld" />
                        <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                            <sup>‚ô†</sup><a href="https://openreview.net/profile?id=~Samin_Mahdizadeh_Sani1" target="_blank">Samin Mahdizadeh Sani</a>*,
                        </span>
                        <span class="author-block">
                            <sup>‚ô†‚ô°</sup><a href="https://kuwingfung.github.io/" target="_blank">Max Ku</a>*,
                        </span>
                        <span class="author-block">
                            <sup>‚ô†</sup>Nima Jamali,
                        </span>
                        <span class="author-block">
                            <sup>‚ô†</sup>Matina Mahdizadeh Sani,
                        </span>
                        <span class="author-block">
                            <sup>‚ô¶</sup>Paria Khoshtab,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup><a href="https://orcid.org/0009-0004-0197-6328" target="_blank">Wei-Chieh Sun</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô¶</sup>Parnian Fazel,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup><a href="https://zrt.wtf/" target="_blank">Zhi Rui Tam</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup><a href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=aV-Ddp4AAAAJ" target="_blank">Thomas Chong</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup>Edisy Kin Wai Chan,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup>Donald Wai Tong Tsang,
                        </span>
                        <span class="author-block">
                            <sup>‚ô¶</sup><a href="https://chiaoweihsu.com/" target="_blank">Chiao-Wei Hsu</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup>Ting Wai Lam,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup><a href="https://scholar.google.com/citations?user=uagptQgAAAAJ&hl=zh-TW" target="_blank">Ho Yin Sam Ng</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô¶</sup>Chiafeng Chu,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup>Chak-Wing Mak,
                        </span>
                        <span class="author-block">
                            <sup>‚ô¶</sup><a href="https://kemingwu.github.io/" target="_blank">Keming Wu</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup><a href="https://scholar.google.com/citations?user=BteRkfIAAAAJ&hl=en" target="_blank">Hiu Tung Wong</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup>Yik Chun Ho,
                        </span>
                        <span class="author-block">
                            <sup>‚ô†</sup>Chi Ruan,
                        </span>
                        <span class="author-block">
                            <sup>‚ô¶</sup><a href="https://zhuofeng-li.github.io/" target="_blank">Zhuofeng Li</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°</sup><a href="https://ishengfang.github.io/" target="_blank">I-Sheng Fang</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô£‚ô°</sup><a href="https://kblueleaf.net/" target="_blank">Shih-Ying Yeh</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô°¬ß</sup><a href="https://hkchengrex.com/" target="_blank">Ho Kei Cheng</a>,
                        </span>
                        <span class="author-block">
                            <sup>‚ô¶</sup>Ping Nie,
                        </span>
                        <span class="author-block">
                            <sup>‚ô†</sup><a href="https://wenhuchen.github.io/" target="_blank">Wenhu Chen</a>
                        </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                        <span class="author-block"><small>
                            <sup>‚ô†</sup>University of Waterloo,
                            <sup>‚ô°</sup><a href="https://github.com/GuidedGenerationGroup" target="_blank">G-G-G</a>,
                            <sup>‚ô£</sup><a href="https://github.com/comfy-org" target="_blank">Comfy Org</a>,
                            <sup>¬ß</sup>University of Illinois Urbana-Champaign,
                            <sup>‚ô¶</sup>Independent
                            </small>
                        </span>
                        <span class="author-block"><small><a href="mailto:samin.mahdizadeh@gmail.com">samin.mahdizadeh@gmail.com</a>,</small></span>
                        <span class="author-block"><small><a href="mailto:m3ku@uwaterloo.ca">m3ku@uwaterloo.ca</a>,</small></span>
                        <span class="author-block"><small><a href="mailto:wenhuchen@uwaterloo.ca">wenhuchen@uwaterloo.ca</a></small></span>
                        </div>


                        <div class="column has-text-centered">
                           <!-- <h2 class="title is-5">ICLR 2024</h2> -->
                            <div class="publication-links">
                                <span class="link-block">
                          <a href="https://huggingface.co/datasets/TIGER-Lab/ImagenWorld-condition-set" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            ü§ó
                          </span>
                                <span>Dataset</span>
                                </a>
                                </span>

                                <!-- Github link -->
                                <span class="link-block">
                      <a href="https://github.com/TIGER-AI-Lab/ImagenWorld" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                                <span>Code</span>
                                </a>
                                </span>


                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                                  <a href="https://github.com/TIGER-AI-Lab/ImagenWorld/blob/a3200b87c1714b106bf2c55daae346634a8e9cbf/static/preprint.pdf" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                      <i class="fas fa-file-alt"></i>
                                    </span>
                                    <span>Preprint</span>
                                  </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://huggingface.co/spaces/TIGER-Lab/ImagenWorld-Visualizer" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                              <span>üñºÔ∏èüèõÔ∏è Visualization</span>
                                              </a>
                                </span>
                           
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
            <div class="container" style="margin-bottom: 2vh;margin-top: -6vh;">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                    <h2 class="title is-3">TL;DR</h2>
                    <div class="content has-text-justified">
                        <p>
                            We build a 3.6K conditions set, 6-task x 6-domain benchmark with 20K explainable human annotations that stress-tests image generation and editing, shows where models break (notably local edits and text-heavy content), benchmarks against VLM-as-judge baselines, and identifies key failure modes.   </p>
                    </div>
                    </div>
                </div>
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Advances in diffusion, autoregressive, and hybrid models have enabled high-quality image synthesis for tasks such as text-to-image, editing, and reference-guided composition. Yet, existing benchmarks remain limited, either focus on isolated tasks, cover only narrow domains, or provide opaque scores without explaining failure modes. We introduce ImagenWorld, a benchmark of 3.6K condition sets spanning six core tasks (generation and editing, with single or multiple references) and six topical domains (artworks, photorealistic images, information graphics, textual graphics, computer graphics, and screenshots). The benchmark is supported by 20K fine-grained human annotations and an explainable evaluation schema that tags localized object-level and segment-level errors, complementing automated VLM-based metrics. Our large-scale evaluation of 14 models yields several insights: (1) models typically struggle more in editing tasks than in generation tasks, especially in local edits. (2) models excel in artistic and photorealistic settings but struggle with symbolic and text-heavy domains such as screenshots and information graphics. (3) closed-source systems lead overall, while targeted data curation (e.g., Qwen-Image) narrows the gap in text-heavy cases. (4) modern VLM-based metrics achieve Kendall accuracies up to 0.79, approximating human ranking, but fall short of fine-grained, explainable error attribution. ImagenWorld provides both a rigorous benchmark and a diagnostic tool to advance robust image generation.   </p>
                </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

        <section class="hero is-light is-small">
        <div class="hero-body has-text-centered">
          <h1 class="title is-1 critique-coder">
            <span class="critique-coder">ImagenWorld</span>
          </h1>
        </div>
    </section>



    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Overview</h2>
                        <div class="item">
                            <p style="margin-bottom: 12px;">We introduce ImagenWorld, a large-scale, human-centric benchmark designed to stress-test image generation models in real-world scenarios. Unlike prior evaluations that focus on isolated tasks or narrow domains, ImagenWorld is organized into six domains: Artworks, Photorealistic Images, Information Graphics, Textual Graphics, Computer Graphics, and Screenshots, and six tasks: Text-to-Image Generation (TIG), Single-Reference Image Generation (SRIG), Multi-Reference Image Generation (MRIG), Text-to-Image Editing (TIE), Single-Reference Image Editing (SRIE), and Multi-Reference Image Editing (MRIE). The benchmark includes 3.6K condition sets and 20K fine-grained human annotations, providing a comprehensive testbed for generative models. To support explainable evaluation, ImagenWorld applies object- and segment-level extraction to generated outputs, identifying entities such as objects and fine-grained regions. This structured decomposition enables human annotators to provide not only scalar ratings but also detailed tags of object-level and segment-level failures.</p>
                            <img src="static/images/overview.PNG" alt="MY ALT TEXT" />
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


        <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Dataset Preview</h2>
                        <h2 class="subtitle">We build a diverse benchmark covering 6 topics x 6 tasks = 36 areas, and we annotated over 20K images in total, each provided object-level and segment-level annotations to localize the errors.</h2>
                        <div class="item">
                            <img src="static/images/dataset.PNG" style="width: 80%; height: auto;" alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 1: Illustrative samples from our dataset across six tasks: Text-to-Image Generation (TIG), Single-Reference Image Generation (SRIG), Multi-Reference Image Generation (MRIG), Text-to-Image Editing (TIE), Single-Reference Image Editing (SRIE), and Multi-Reference Image Editing (MRIE). For each task, we show both a successful generation (green) and a failure case (red).
                            </h2>

                            <img src="static/images/explain-tag.PNG" style="width: 70%; height: auto;" alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 2: Examples include object-level issues, where expected objects are missing or distorted, and segment-level issues, where annotated regions highlight specific regions with visual inconsistencies that affect evaluation scores.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Overall Results</h2>
                        <div class="item">
                            <ul style="margin-bottom: 14px; list-style: disc; list-style-position: outside; padding-left: 1.25rem; text-align: left;">
                            <li>
                                Across ImagenWorld, models struggle primarily with editing (TIE/SRIE/MRIE) compared to generation (TIG/SRIG/MRIG). 
                                Performance peaks on Artworks and Photorealistic images, whereas Information Graphics and Screenshots are most 
                                challenging due to their symbolic content and text-heavy, structured layouts.
                            </li>
                            </ul>
                            <img src="static/images/metric.PNG"  alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 3: Mean human evaluation scores across our four metrics by topic (left) and task (right).
                            </h2>

                            <img src="static/images/unified.PNG" style="width: 90%; height: auto;" alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 4: Overall human rating by task and topic for the four unified models that support all six tasks.
                            </h2>

                            <ul style="margin-bottom: 14px; list-style: disc; list-style-position: outside; padding-left: 1.25rem; text-align: left;">
                            <li>
                               Models struggle to execute localized edits reliably: AR‚Äìdiffusion hybrids often overwrite the input with a new image, while diffusion editors struggle in the opposite way, frequently doing nothing and leaving the input unchanged.
                            </li>
                            </ul>

                            <img src="static/images/arch.png" style="width: 90%; height: auto;" alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 5: Percentage of cases where the model generating completely new image or simply return input in image editing tasks.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

        <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Common Failure Modes</h2>
                        <div class="item">
                            <h2><p><b>1. Failing to Precisely Follow Instructions</b></p></h2>
                            <p>Prompt:</p>
                            <p>Edit image 1. Replace the top-left crate with the yellow warning sign from image 3. Place the pink crewmate (from the center of image 2) and the yellow crewmate (from the bottom right of image 2) standing side-by-side on the central doorway in image 1. Ensure all new elements are integrated with correct perspective, lighting, and scale.</p>
                            <img src="static/images/fail1.PNG"  alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 6: Instruction-following problem: The model placed red and green crewmates instead of pink and yellow, and the yellow sign‚Äôs position does not match the request.

                            <h2><p><b>2. Numerical Inconsistencies</b></p></h2>
                            <img src="static/images/fail2.PNG"  alt="MY ALT TEXT" />
                            <h2 class="subtitle" style="text-align:center;">
                                Figure 7: Examples of numerical inconsistencies.

                            <h2><p><b>3. Segments and Labeling Issues</b></p></h2>
                            <img src="static/images/fail3.PNG"  alt="MY ALT TEXT" />
                            <h2 class="subtitle" style="text-align:center;">
                                Figure 8: Examples of labeling issues.
                            </h2>

                            <h2><p><b>4. Generating New Image in Editing</b></p></h2>
                            <img src="static/images/fail6.PNG"  alt="MY ALT TEXT" />
                            <h2 class="subtitle" style="text-align:center;">
                                Figure 9: Examples of generating a new image when the task is editing.
                            </h2>

                            <h2><p><b>5. Plots and chart errors</b></p></h2>
                            <img src="static/images/fail4.PNG"  alt="MY ALT TEXT" />
                            <h2 class="subtitle" style="text-align:center;">
                                Figure 10: Examples of plots and diagram issues.
                            </h2>

                            <h2><p><b>6. Unreadable Text</b></p></h2>
                            <img src="static/images/fail5.PNG"  alt="MY ALT TEXT" />
                            <h2 class="subtitle"style="text-align:center;" >
                                Figure 11: Examples of text issues.
                            </h2>

                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>




    <!-- BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Citation</h2>
            Please kindly cite our paper if you use our code, data, models or results:
            <br><br>
            <pre><code>@inproceedings{}</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a>                            project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                                target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>
<style>
    .buttonGroup {
        text-align: center;
    }
    
    .buttonGroup>button {
        padding: 15px;
        color: white;
        background-color: #363636;
        border-radius: 5px;
    }
    
    .buttonGroup>button:hover {
        box-shadow: 5px;
    }
</style>

</html>
